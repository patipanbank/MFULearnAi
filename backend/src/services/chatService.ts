import { ChatModel, Chat, ChatMessage } from '../models/chat';
import { wsManager } from '../utils/websocketManager';
import { agentService } from './agentService';
import { usageService } from './usageService';
import { getLLM } from '../agent/llmFactory';
import { toolRegistry, createMemoryTool, ToolFunction } from '../agent/toolRegistry';
import { createPromptTemplate } from '../agent/promptFactory';
import { createAgent } from '../agent/agentFactory';
import { redis } from '../lib/redis';
import { memoryService } from './memoryService';

export class ChatService {
  constructor() {
    console.log('‚úÖ Chat service initialized');
  }

  public async createChat(userId: string, name: string, agentId?: string): Promise<Chat> {
    const chat = new ChatModel({
      userId,
      name,
      messages: [],
      agentId,
      isPinned: false,
      createdAt: new Date(),
      updatedAt: new Date()
    });

    await chat.save();
    console.log(`‚úÖ Created chat session ${chat._id} for user ${userId}`);
    return chat;
  }

  public async getChat(chatId: string, userId: string): Promise<Chat | null> {
    console.log(`üîç Looking for chat: ${chatId} for user: ${userId}`);
    
    const chat = await ChatModel.findOne({ _id: chatId, userId });
    
    if (chat) {
      console.log(`‚úÖ Found chat: ${chatId}`);
    } else {
      console.log(`‚ùå Chat not found: ${chatId}`);
      // Let's also check if the chat exists without user filter
      const chatWithoutUser = await ChatModel.findById(chatId);
      if (chatWithoutUser) {
        console.log(`‚ö†Ô∏è Chat exists but belongs to user: ${chatWithoutUser.userId}`);
      } else {
        console.log(`‚ùå Chat doesn't exist in database: ${chatId}`);
      }
    }
    
    return chat;
  }

  public async addMessage(chatId: string, message: Omit<ChatMessage, 'id' | 'timestamp'>): Promise<ChatMessage> {
    const chat = await ChatModel.findById(chatId);
    if (!chat) {
      throw new Error(`Chat session ${chatId} not found`);
    }

    // Ensure content is not empty
    if (!message.content || message.content.trim() === '') {
      message.content = '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•...';
    }

    const newMessage: ChatMessage = {
      id: Math.random().toString(36).substr(2, 9),
      ...message,
      timestamp: new Date()
    };

    chat.messages.push(newMessage);
    chat.updatedAt = new Date();
    await chat.save();

    console.log(`‚úÖ Added message to session ${chatId}`);

    return newMessage;
  }

  public async processMessage(chatId: string, userId: string, content: string, images?: Array<{ url: string; mediaType: string }>): Promise<void> {
    try {
      // Add user message first (like in legacy)
      const userMessage = await this.addMessage(chatId, {
        role: 'user',
        content,
        images
      });

      // Get chat and agent info
      const chat = await ChatModel.findById(chatId);
      if (!chat) {
        throw new Error(`Chat session ${chatId} not found`);
      }

      // Get agent configuration if available
      let agentConfig = null;
      let modelId: string | null = null;
      let collectionNames: string[] = [];
      let systemPrompt: string | null = null;
      let temperature = 0.7;
      let maxTokens = 4000;

      if (chat.agentId) {
        agentConfig = await agentService.getAgentById(chat.agentId);
        if (agentConfig) {
          modelId = agentConfig.modelId;
          collectionNames = agentConfig.collectionNames || [];
          systemPrompt = agentConfig.systemPrompt;
          temperature = agentConfig.temperature;
          maxTokens = agentConfig.maxTokens;
        }
      }

      // Process with AI agent using legacy-style processing (no placeholder message)
      await this.processWithAILegacy(chatId, content, images, {
        modelId,
        collectionNames,
        systemPrompt,
        temperature,
        maxTokens,
        agentId: chat.agentId
      }, userId);

    } catch (error) {
      console.error('‚ùå Error processing message:', error);
      
      // Handle validation errors specifically
      if (error instanceof Error && error.message.includes('validation failed')) {
        console.error('Validation error details:', error);
        
        // Try to fix the validation issue by updating the message
        try {
          await ChatModel.updateOne(
            { _id: chatId, 'messages.content': { $exists: false } },
            { $set: { 'messages.$.content': '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•...' } }
          );
        } catch (fixError) {
          console.error('Failed to fix validation error:', fixError);
        }
      }
      
      // Send error to client
      if (wsManager.getSessionConnectionCount(chatId) > 0) {
        wsManager.broadcastToSession(chatId, JSON.stringify({
          type: 'error',
          data: 'Failed to process message'
        }));
      }
    }
  }

  private async processWithAILegacy(chatId: string, userMessage: string, images?: Array<{ url: string; mediaType: string }>, config?: {
    modelId?: string | null;
    collectionNames?: string[];
    systemPrompt?: string | null;
    temperature?: number;
    maxTokens?: number;
    agentId?: string;
  }, userId?: string): Promise<void> {
    try {
      // Simulate thinking time
      await this.delay(1000);

      // === LangChain agent graph logic ===
      const agentId = config?.agentId;
      if (!agentId) throw new Error('No agentId');
      // 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° LLM instance
      const llm = getLLM(config?.modelId || 'anthropic.claude-3-5-sonnet-20240620-v1:0', {
        temperature: config?.temperature,
        maxTokens: config?.maxTokens
      });
      // 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° tools (‡∏£‡∏ß‡∏° memory tool)
      const sessionTools = createMemoryTool(chatId);
      // map ToolMeta ‡πÄ‡∏õ‡πá‡∏ô ToolFunction
      const allTools: { [name: string]: ToolFunction } = {};
      for (const [k, v] of Object.entries(toolRegistry)) allTools[k] = v.func;
      for (const [k, v] of Object.entries(sessionTools)) allTools[k] = v.func;
      // 3. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° prompt template
      const promptTemplate = createPromptTemplate(config?.systemPrompt || '', true);
      // 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á agent executor
      const agent = createAgent(llm, allTools, config?.systemPrompt || '');
      // 5. ‡∏ß‡∏ô loop agent graph (multi-turn)
      // ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
      const chatFromDb = await ChatModel.findById(chatId);
      if (!chatFromDb) {
        throw new Error(`Chat session ${chatId} not found during AI processing`);
      }
      let messages: ChatMessage[] = chatFromDb.messages.map(msg => ({
        role: msg.role,
        content: msg.content,
        id: msg.id,
        timestamp: msg.timestamp
      }));
      // ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏õ (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ agent ‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏à‡∏≤‡∏Å prompt ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å messages ‡∏ô‡∏µ‡πâ)
      // ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏±‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏°‡∏∑‡πà‡∏≠ LLM ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
      const userMessageInDb = messages[messages.length - 1];
      messages = messages.slice(0, messages.length - 1);
      // ‡πÄ‡∏û‡∏¥‡πà‡∏° userMessageInDb ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤ messages (user message ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
      messages.push(userMessageInDb);

      // ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏≤ Redis (recent history)
      await memoryService.addRecentMessage(chatId, userMessageInDb);

      // ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ù‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏á long-term memory (ChromaDB) ‡∏ó‡∏∏‡∏Å‡πÜ 10 ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
      const currentMessageCount = chatFromDb.messages.length;
      if (this.shouldEmbedMessages(currentMessageCount)) {
        // Note: ‡πÉ‡∏´‡πâ LLM ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å tool memory_embed ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings
      }

      // ‡∏™‡∏£‡πâ‡∏≤‡∏á assistant message ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤ messages
      const assistantMessage = await this.addMessage(chatId, {
        role: 'assistant',
        content: '',
      });
      messages.push(assistantMessage);
      let fullContent = '';
      let turn = 0;
      let done = false;
      while (!done && turn < 5) {
        turn++;
        // 5.1 ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt
        const prompt = promptTemplate(messages);
        // 5.2 ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡πÅ‡∏ö‡∏ö streaming
        let output = '';
        let isToolCall = false;
        let toolMatch: RegExpMatchArray | null = null;
        let chunkBuffer = '';
        try {
          for await (const chunk of llm.stream(prompt)) {
            chunkBuffer += chunk;
            toolMatch = chunkBuffer.match(/\[TOOL:([\w_]+)\](.*)/s);
            if (toolMatch) {
              isToolCall = true;
              break;
            }
            output += chunk;
            fullContent += chunk;
            await ChatModel.updateOne(
              { _id: chatId, 'messages.id': assistantMessage.id },
              { $set: { 'messages.$.content': fullContent, updatedAt: new Date() } }
            );
            if (wsManager.getSessionConnectionCount(chatId) > 0) {
              wsManager.broadcastToSession(chatId, JSON.stringify({
                type: 'chunk',
                data: chunk
              }));
            }
          }
        } catch (err) {
          console.error('‚ùå Error during LLM streaming:', err);
          break;
        }
        // 5.3 Parse output: ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ [TOOL:name] ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å tool ‡∏ô‡∏±‡πâ‡∏ô
        if (isToolCall && toolMatch) {
          const toolName = toolMatch[1];
          const toolInput = toolMatch[2]?.trim() || '';
          if (allTools[toolName]) {
            // Broadcast tool_start event
            if (wsManager.getSessionConnectionCount(chatId) > 0) {
              wsManager.broadcastToSession(chatId, JSON.stringify({
                type: 'tool_start',
                data: {
                  tool_name: toolName,
                  tool_input: toolInput
                }
              }));
            }

            const toolResult = await allTools[toolName](toolInput, chatId);
            // Broadcast tool_result event
            if (wsManager.getSessionConnectionCount(chatId) > 0) {
              wsManager.broadcastToSession(chatId, JSON.stringify({
                type: 'tool_result',
                data: {
                  tool_name: toolName,
                  output: toolResult
                }
              }));
            }
            // ‡πÄ‡∏û‡∏¥‡πà‡∏° tool result message (user) ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÄ‡∏Ç‡πâ‡∏≤ messages
            const toolResultMessage = await this.addMessage(chatId, {
              role: 'user',
              content: `[TOOL_RESULT:${toolName}] ${toolResult}`,
            });
            messages.push(toolResultMessage);
            continue;
          } else {
            messages.push({ ...assistantMessage, content: `[Error: Tool ${toolName} not found]` });
            done = true;
            fullContent += `[Error: Tool ${toolName} not found]`;
            break;
          }
        } else {
          // ‡πÑ‡∏°‡πà‡∏°‡∏µ tool call, ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô final answer
          messages.push({ ...assistantMessage, content: output });
          fullContent += output;
          done = true;
        }
        // ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å LLM ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏´‡∏£‡∏∑‡∏≠ tool ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô: ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Redis ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢
        const lastMessage = messages[messages.length - 1];
        if (lastMessage && lastMessage.role === 'assistant' && lastMessage.content) {
          await memoryService.addRecentMessage(chatId, lastMessage);
        } else if (lastMessage && lastMessage.role === 'user' && lastMessage.content && lastMessage.content.startsWith('[TOOL_RESULT:')) {
          await memoryService.addRecentMessage(chatId, lastMessage);
        }
      }
      // Calculate tokens and update usage
      const inputTokens = Math.floor(userMessage.length / 4);
      const outputTokens = Math.floor(fullContent.length / 4);
      if (userId) {
        await usageService.updateUsage(userId, inputTokens, outputTokens);
      }
      if (wsManager.getSessionConnectionCount(chatId) > 0) {
        wsManager.broadcastToSession(chatId, JSON.stringify({
          type: 'end',
          data: {
            sessionId: chatId,
            inputTokens,
            outputTokens
          }
        }));
      }
    } catch (error) {
      console.error('‚ùå Error in AI processing:', error);
      if (wsManager.getSessionConnectionCount(chatId) > 0) {
        wsManager.broadcastToSession(chatId, JSON.stringify({
          type: 'error',
          data: 'Failed to process AI response'
        }));
      }
    }
  }





  private async streamResponse(chatId: string, messageId: string, response: string): Promise<void> {
    const words = response.split(' ');
    let fullContent = '';
    
    for (let i = 0; i < words.length; i++) {
      const chunk = (i > 0 ? ' ' : '') + words[i];
      fullContent += chunk;
      
      // Update message content in database
      await ChatModel.updateOne(
        { _id: chatId, 'messages.id': messageId },
        { 
          $set: { 
            'messages.$.content': fullContent,
            updatedAt: new Date()
          }
        }
      );
      
      if (wsManager.getSessionConnectionCount(chatId) > 0) {
        wsManager.broadcastToSession(chatId, JSON.stringify({
          type: 'chunk',
          data: chunk
        }));
      }

      await this.delay(100);
    }
  }

  private async streamResponseLegacy(chatId: string, response: string): Promise<void> {
    const words = response.split(' ');
    let fullContent = '';
    
    // Create assistant message first (like in legacy)
    const assistantMessage = await this.addMessage(chatId, {
      role: 'assistant',
      content: ''
    });
    
    for (let i = 0; i < words.length; i++) {
      const chunk = (i > 0 ? ' ' : '') + words[i];
      fullContent += chunk;
      
      // Update message content in database
      await ChatModel.updateOne(
        { _id: chatId, 'messages.id': assistantMessage.id },
        { 
          $set: { 
            'messages.$.content': fullContent,
            updatedAt: new Date()
          }
        }
      );
      
      if (wsManager.getSessionConnectionCount(chatId) > 0) {
        wsManager.broadcastToSession(chatId, JSON.stringify({
          type: 'chunk',
          data: chunk
        }));
      }

      await this.delay(100);
    }
  }

  private generateResponse(userMessage: string, images?: Array<{ url: string; mediaType: string }>, config?: {
    modelId?: string | null;
    collectionNames?: string[];
    systemPrompt?: string | null;
    temperature?: number;
    maxTokens?: number;
    agentId?: string;
  }): string {
    // Use system prompt if available
    if (config?.systemPrompt) {
      const responses = [
        `‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö: ${config.systemPrompt}\n\n‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° "${userMessage}" ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:`,
        `‡∏ï‡∏≤‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Ç‡∏≠‡∏á AI Assistant: ${config.systemPrompt}\n\n‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö "${userMessage}":`
      ];
      
      const baseResponse = responses[Math.floor(Math.random() * responses.length)];
      return `${baseResponse} ${this.generateDetailedResponse()}`;
    }

    // Default responses
    const responses = [
      `‡∏â‡∏±‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö "${userMessage}" ‡πÅ‡∏•‡πâ‡∏ß ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°:`,
      `‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° "${userMessage}" ‡∏â‡∏±‡∏ô‡∏à‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏ü‡∏±‡∏á:`,
      `‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° "${userMessage}" ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:`,
      `‡∏â‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° "${userMessage}" ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏•‡∏∞‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏â‡∏±‡∏ô‡∏û‡∏ö:`
    ];

    const baseResponse = responses[Math.floor(Math.random() * responses.length)];
    
    if (images && images.length > 0) {
      return `${baseResponse} ‡∏â‡∏±‡∏ô‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÅ‡∏ô‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢ ‡∏â‡∏±‡∏ô‡∏à‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î. ${this.generateDetailedResponse()}`;
    }

    return `${baseResponse} ${this.generateDetailedResponse()}`;
  }

  private generateDetailedResponse(): string {
    const details = [
      `‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡πÑ‡∏õ‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏î‡πâ.`,
      `‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏±‡∏á‡πÄ‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢.`,
      `‡∏â‡∏±‡∏ô‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô.`,
      `‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡πÑ‡∏´‡∏ô‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÅ‡∏à‡πâ‡∏á‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô‡∏ó‡∏£‡∏≤‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏â‡∏±‡∏ô‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°.`
    ];

    return details[Math.floor(Math.random() * details.length)];
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  public async getUserChats(userId: string): Promise<Chat[]> {
    const chats = await ChatModel.find({ userId })
      .sort({ isPinned: -1, updatedAt: -1 })
      .exec();

    return chats;
  }

  public async deleteChat(chatId: string, userId: string): Promise<boolean> {
    const result = await ChatModel.deleteOne({ _id: chatId, userId });
    const success = result.deletedCount > 0;
    
    if (success) {
      console.log(`üóëÔ∏è Deleted chat session ${chatId}`);
    }
    
    return success;
  }

  public async updateChatName(chatId: string, userId: string, name: string): Promise<Chat | null> {
    const chat = await ChatModel.findOneAndUpdate(
      { _id: chatId, userId },
      { name, updatedAt: new Date() },
      { new: true }
    );

    if (chat) {
      console.log(`‚úèÔ∏è Updated chat name for session ${chatId}`);
    }
    
    return chat;
  }

  public async updateChatPinStatus(chatId: string, userId: string, isPinned: boolean): Promise<Chat | null> {
    const chat = await ChatModel.findOneAndUpdate(
      { _id: chatId, userId },
      { isPinned, updatedAt: new Date() },
      { new: true }
    );

    if (chat) {
      console.log(`üìå Updated pin status for session ${chatId}: ${isPinned}`);
    }
    
    return chat;
  }

  public async clearChatMemory(chatId: string): Promise<void> {
    try {
      // Clear Redis memory
      await redis.del(`chat:history:${chatId}`);
      // Clear memory tool (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
      if (typeof (global as any).clearChatMemoryTool === 'function') {
        await (global as any).clearChatMemoryTool(chatId);
      }
      console.log(`‚úÖ Memory cleared for chat ${chatId}`);
    } catch (error) {
      console.error(`‚ùå Failed to clear memory for chat ${chatId}:`, error);
    }
  }

  private async getRecentMessagesFromRedis(chatId: string): Promise<any[]> {
    try {
      const data = await redis.get(`chat:history:${chatId}`);
      if (!data) return [];
      return JSON.parse(data);
    } catch (e) {
      return [];
    }
  }

  private async setRecentMessagesToRedis(chatId: string, messages: any[]): Promise<void> {
    try {
      await redis.set(`chat:history:${chatId}`,
        JSON.stringify(messages), 'EX', 86400 // TTL 24 ‡∏ä‡∏°.
      );
    } catch (e) {
      // fallback ‡πÄ‡∏á‡∏µ‡∏¢‡∏ö ‡πÜ
    }
  }

  private async restoreRecentContextIfNeeded(chatId: string, allMessages: any[]): Promise<void> {
    const redisMessages = await this.getRecentMessagesFromRedis(chatId);
    if (!redisMessages || redisMessages.length === 0) {
      // Restore recent 10 messages
      const recent = allMessages.slice(-10);
      await this.setRecentMessagesToRedis(chatId, recent);
      console.log(`üîÑ Restored recent context to Redis for chat ${chatId}`);
    }
  }

  private async embedMessagesIfNeeded(chatId: string, allMessages: any[]): Promise<void> {
    if (allMessages.length % 10 === 0 && allMessages.length > 0) {
      // ‡∏ù‡∏±‡∏á embedding ‡∏ó‡∏∏‡∏Å 10 ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (mock call memory tool)
      if (typeof (global as any).addChatMemoryTool === 'function') {
        await (global as any).addChatMemoryTool(chatId, allMessages);
        console.log(`üìö Embedded ${allMessages.length} messages to memory tool for chat ${chatId}`);
      }
    }
  }

  private shouldUseMemoryTool(messageCount: number): boolean {
    // Use memory tool when there are more than 10 messages
    return messageCount > 10;
  }

  private shouldUseRedisMemory(messageCount: number): boolean {
    // Always use Redis memory for recent conversations (last 10 messages)
    return true;
  }

  private shouldEmbedMessages(messageCount: number): boolean {
    // Embed messages every 10 messages (10, 20, 30, etc.)
    return messageCount % 10 === 0;
  }

  public getStats(): any {
    return {
      totalChats: 0, // TODO: Implement actual stats
      activeSessions: 0, // TODO: Implement active session count
      totalMessages: 0
    };
  }
}

export const chatService = new ChatService(); 